# parse_tool.py
import os
import re
import math
import cantools
import pandas as pd
import csv
from tkinter import filedialog, Tk
from tqdm import tqdm
from collections import OrderedDict

# ---------------------- TRC MERGE & INFO EXTRACTION ---------------------- #
def extract_trc_info(filepath):
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()

    file_version = None
    start_timestamp = None
    start_time_str = None
    header = []
    messages = []
    in_header = True

    for line in lines:
        if line.startswith(";$FILEVERSION="):
            file_version = line.split("=")[1].strip()
        elif line.startswith(";$STARTTIME="):
            try:
                start_timestamp = float(line.split("=")[1].strip())
            except ValueError:
                pass
        elif line.strip().startswith(";   Start time:"):
            start_time_str = line.strip().split(": ", 1)[1].strip()

        if in_header:
            header.append(line)
            if line.strip().startswith(";---+"):
                in_header = False
        else:
            messages.append(line)

    if not file_version or start_timestamp is None:
        raise ValueError(f"Missing version or start time in: {filepath}")

    return {
        "file": filepath,
        "filename": os.path.basename(filepath),
        "version": file_version,
        "start_timestamp": start_timestamp,
        "start_time_str": start_time_str,
        "header": header,
        "messages": messages
    }


def merge_in_forced_order(trc_files):
    if len(trc_files) == 1:
        print("âœ… Single TRC file provided. Skipping merge.")
        return trc_files[0]

    file_infos = [extract_trc_info(f) for f in trc_files]
    versions = set(info["version"] for info in file_infos)
    if len(versions) > 1:
        raise ValueError("âŒ Cannot merge mixed TRC versions. Ensure all are version 1.1 or 2.0.")

    file_infos.sort(key=lambda x: x["start_timestamp"])
    print("\nðŸ•’ File Start Times (merge will follow this order):")
    for info in file_infos:
        print(f"- {info['filename']:20} â†’ $STARTTIME = {info['start_timestamp']} â†’ {info['start_time_str']}")

    primary_info = file_infos[0]
    primary_header = primary_info["header"]
    primary_start_timestamp = primary_info["start_timestamp"]
    primary_start_time_str = primary_info["start_time_str"]

    final_lines = []
    line_counter = 1
    global_start_time = primary_start_timestamp

    for info in file_infos:
        matched = 0
        for line in info["messages"]:
            match = re.search(
                r'^\s*\d+\)\s+([\d.]+)\s+(Rx|Tx)\s+([0-9A-Fa-f]+)\s+\d+\s+((?:[0-9A-Fa-f]{2}\s*)+)',
                line
            )
            if match:
                offset_ms = float(match.group(1))
                abs_time = info["start_timestamp"] + (offset_ms / 1000.0)
                new_offset_ms = (abs_time - global_start_time) * 1000
                new_offset_str = f"{new_offset_ms:10.1f}"
                new_line = re.sub(r'^\s*\d+\)\s+[\d.]+', f"{line_counter:6d}){new_offset_str}", line.strip())
                final_lines.append(new_line)
                line_counter += 1
                matched += 1
        print(f"âœ… {info['filename']} â€” matched {matched} of {len(info['messages'])} lines")

    if not final_lines:
        raise ValueError("âŒ Merge failed: No TRC messages extracted.")

    output_path = os.path.join(os.path.dirname(trc_files[0]), "Final_Merge_trc.trc")
    with open(output_path, "w", encoding="utf-8") as f:
        for line in primary_header:
            if line.startswith(";$STARTTIME="):
                f.write(f";$STARTTIME={primary_start_timestamp}\n")
            elif line.strip().startswith(";   Start time:"):
                f.write(f";   Start time: {primary_start_time_str}\n")
            elif line.strip().startswith(";   Generated by"):
                f.write(";   Merged by TRC Tool\n")
            else:
                f.write(line)
        f.write("\n")
        for line in final_lines:
            f.write(line + "\n")

    print(f"\nâœ… Merged TRC saved at: {output_path}")
    return output_path


# ---------------------- TRC PARSING & DECODING ---------------------- #
def parse_trc_file(trc_file, dbc):
    signal_names = set()
    decoded_rows = []
    last_known_values = {}
    file_version = None

    with open(trc_file, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()

    for line in lines:
        if line.startswith(";$FILEVERSION="):
            file_version = line.split("=")[1].strip()
            break

    for line in tqdm(lines, desc="ðŸ” Decoding", unit="lines"):
        try:
            if file_version == "1.1":
                match = re.search(
                    r'^\s*\d+\)\s+([\d.]+)\s+(Rx|Tx)\s+([0-9A-Fa-f]+)\s+\d+\s+((?:[0-9A-Fa-f]{2}\s*)+)',
                    line
                )
                if not match:
                    continue
                timestamp = float(match.group(1)) / 1000
                can_id = int(match.group(3), 16)
                data_bytes = bytes(int(b, 16) for b in match.group(4).split())

            elif file_version == "2.0":
                match = re.search(
                    r'^\s*\d+\s+([\d.]+)\s+\S+\s+([0-9A-Fa-f]+)\s+(Rx|Tx)\s+\d+\s+((?:[0-9A-Fa-f]{2}\s*)+)',
                    line
                )
                if not match:
                    continue
                timestamp = float(match.group(1)) / 1000
                can_id = int(match.group(2), 16)
                data_bytes = bytes(int(b, 16) for b in match.group(4).split())
            else:
                print("âŒ Unsupported TRC file version.")
                return [], []

            message = dbc.get_message_by_frame_id(can_id)
            if not message:
                continue

            decoded = message.decode(data_bytes)
            signal_names.update(decoded.keys())
            last_known_values.update(decoded)

            row = {"Time (s)": round(timestamp, 6)}
            row.update(last_known_values)
            decoded_rows.append(row)

        except Exception:
            continue

    return decoded_rows, ["Time (s)"] + sorted(signal_names)


def write_large_csv(df, base_path):
    row_limit = 1_000_000
    total_rows = len(df)
    total_parts = math.ceil(total_rows / row_limit)
    paths = []

    print(f"\nðŸ’¾ Writing decoded data to CSV ({total_parts} part(s))...")
    for i in range(total_parts):
        chunk = df.iloc[i * row_limit: (i + 1) * row_limit]
        suffix = "" if i == 0 else f"_part{i+1}"
        path = f"{base_path}{suffix}.csv"
        chunk.to_csv(path, index=False)
        paths.append(path)
        print(f"âœ… Saved: {path}")

    return paths


def trc_to_csv():
    print("ðŸ“‚ Select one or more .trc files")
    trc_files = filedialog.askopenfilenames(filetypes=[("TRC files", "*.trc")])
    if not trc_files:
        print("âŒ No TRC files selected.")
        return

    try:
        merged_path = merge_in_forced_order(trc_files)
    except Exception as e:
        print(f"âŒ Merge failed: {e}")
        return

    print("\nðŸ“ Please select the .dbc file")
    dbc_file = filedialog.askopenfilename(filetypes=[("DBC files", "*.dbc")])
    if not dbc_file:
        print("âŒ No DBC file selected.")
        return

    try:
        dbc = cantools.database.load_file(dbc_file)
    except Exception as e:
        print(f"âŒ Failed to load DBC: {e}")
        return

    print("\nðŸ” Decoding merged TRC file...")
    rows, columns = parse_trc_file(merged_path, dbc)

    if not rows:
        print("âŒ No data decoded.")
        return

    df = pd.DataFrame(rows)
    df = df.reindex(columns=columns)

    base_path = os.path.splitext(merged_path)[0] + "_decoded"
    write_large_csv(df, base_path)


# ---------------------- LOG PARSING ---------------------- #
def select_file(title, filetypes):
    root = Tk()
    root.withdraw()
    path = filedialog.askopenfilename(title=title, filetypes=filetypes)
    root.destroy()
    return path


def save_file(title, defaultextension=".csv", filetypes=[("CSV files", "*.csv")]):
    root = Tk()
    root.withdraw()
    path = filedialog.asksaveasfilename(title=title, defaultextension=defaultextension, filetypes=filetypes)
    root.destroy()
    return path


def parse_log_to_compact_csv(log_path, dbc_path, output_csv_path):
    db = cantools.database.load_file(dbc_path)
    message_map = {msg.frame_id: msg for msg in db.messages}

    rows = OrderedDict()  # timestamp -> {signal: value}
    last_known = {}       # signal -> latest value

    with open(log_path, 'r') as f:
        for line in f:
            if not re.match(r"^\d{2}:\d{2}:\d{2}:\d{4}", line.strip()):
                continue
            try:
                parts = line.strip().split()
                timestamp = parts[0]
                can_id = int(parts[3], 16)
                dlc = int(parts[5])
                data = bytes(int(b, 16) for b in parts[6:6 + dlc])

                if can_id not in message_map:
                    continue

                msg = message_map[can_id]
                decoded = msg.decode(data)

                for sig, val in decoded.items():
                    last_known[sig] = val

                snapshot = rows.get(timestamp, last_known.copy())
                snapshot.update(decoded)
                rows[timestamp] = snapshot.copy()

            except Exception:
                continue

    all_signals = sorted(set(sig for snapshot in rows.values() for sig in snapshot))
    headers = ["Timestamp"] + all_signals

    with open(output_csv_path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for ts, snapshot in rows.items():
            row = {"Timestamp": ts}
            row.update({sig: snapshot.get(sig, "") for sig in all_signals})
            writer.writerow(row)

    print(f"âœ… Final snapshot CSV written to {output_csv_path}")


if __name__ == "__main__":
    log_file = select_file("Select your .log file", [("Log Files", "*.log")])
    dbc_file = select_file("Select your .dbc file", [("DBC Files", "*.dbc")])
    csv_output = save_file("Save Clean CSV Output")

    if log_file and dbc_file and csv_output:
        parse_log_to_compact_csv(log_file, dbc_file, csv_output)
